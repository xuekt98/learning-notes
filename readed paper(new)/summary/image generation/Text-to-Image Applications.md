# Part 1 Personalization

#### 1-001 [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/pdf/2208.01618)

**ICLR 2023**

![img](res/T2I%20Applications/1-001-1.png)

Texutal Inversion used one specified symbol to represent the given image and optimize the representation of this symbol for a given group of image, so that the symbol can represent the specified content of the given image.  

</br>


#### 1-002 [PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-002-1.png)

PIA a Personalized Image Animator that excels in aligning with condition images achieving motion controllability by text and the compatibility with various personalized T2I models without specific tuning. 

</br>


#### 1-003 [Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Prompt-Free_Diffusion_Taking_Text_out_of_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-003-1.png)
![img](res/T2I%20Applications/1-003-2.png)

Prompt-Free Diffusion improves image generation quality by eliminating the need for prompts, thereby simplifying the personalized generation process.

</br>


#### 1-004 [PALP: Prompt Aligned Personalization of Text-to-Image Models](https://dl.acm.org/doi/pdf/10.1145/3680528.3687604)

**SIGGRAPH Asia 2024**

![img](res/T2I%20Applications/1-004-1.png)
![img](res/T2I%20Applications/1-004-2.png)

PALP maintains consistency between text and generated images, even under complex prompting conditions, demonstrating excellent performance.

</br>


#### 1-005 [PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PhotoMaker_Customizing_Realistic_Human_Photos_via_Stacked_ID_Embedding_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-005-1.png)
![img](res/T2I%20Applications/1-005-2.png)

PhotoMaker efficiently encodes the input ID images using stacked ID embeddings, ensuring the retention of identity information.  

</br>


#### 1-006 [Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Learning_Disentangled_Identifiers_for_Action-Customized_Text-to-Image_Generation_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-006-1.png)
![img](res/T2I%20Applications/1-006-2.png)

ADI introduces disentangled identifiers, which enable the generation of new images with shared actions through specific symbols, enhancing the diversity of output.

</br>


#### 1-007 [DreamTuner: Single Image is Enough for Subject-Driven Generation](https://arxiv.org/pdf/2312.13691)

**Arxiv 2023**

![img](res/T2I%20Applications/1-007-1.png)
![img](res/T2I%20Applications/1-007-2.png)

DreamTurner achieves theme-driven image generation through a theme encoder, allowing for more detailed preservation of subject identity.

</br>


#### 1-008 [ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf)

**ICCV 2023**

![img](res/T2I%20Applications/1-008-1.png)
![img](res/T2I%20Applications/1-008-2.png)

ELITE introduces a combination of global and local mapping networks to enable fast and accurate customized generation.

</br>


#### 1-009 [Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Tailored_Visions_Enhancing_Text-to-Image_Generation_with_Personalized_Prompt_Rewriting_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-009-1.png)
![img](res/T2I%20Applications/1-009-2.png)

Tailored Visions enhances personalization by rewriting user prompts based on historical interactions.

</br>


#### 1-010 [LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf)

**CVPR 2023**

![img](res/T2I%20Applications/1-010-1.png)
![img](res/T2I%20Applications/1-010-2.png)

Layout Diffusion proposed to construct a structural image patch with region information and transform the patched image into a special layout ot fuse with the normal layout in a unified form. It also proposed a novel Layout Fusion Module(LFM) and Object-aware Cross Attention. 

</br>


#### 1-011 [HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Ruiz_HyperDreamBooth_HyperNetworks_for_Fast_Personalization_of_Text-to-Image_Models_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-011-1.png)
![img](res/T2I%20Applications/1-011-2.png)
![img](res/T2I%20Applications/1-011-3.png)

HyperDreamBooth uses a hypernetwork to generate personalized weights from a single image, allowing for efficient style and context switching.  

</br>


#### 1-012 [Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance](https://openaccess.thecvf.com/content/CVPR2024/papers/Chan_Improving_Subject-Driven_Image_Synthesis_with_Subject-Agnostic_Guidance_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-012-1.png)
![img](res/T2I%20Applications/1-012-2.png)

SAG employed dual classifier-free guidance to ensure that generated outputs align with both themes and input text prompts, enhancing generation accuracy

</br>


#### 1-013 [Personalized Residuals for Concept-Driven Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Ham_Personalized_Residuals_for_Concept-Driven_Text-to-Image_Generation_CVPR_2024_paper.pdf)

**CVPR 2024**

![img](res/T2I%20Applications/1-013-1.png)
![img](res/T2I%20Applications/1-013-2.png)

Personalized Residuals freezed the weights of a pretrained text-conditional diffusion model and learned low-rank residuals for a small subset of the model's layers. The residual-based approach then directly enables application of our proposed sampling technique, which applies the learned residuals only in areas where the concept is localized via cross-attention and applies the original diffusion weights in all other regions.  

</br>


#### 1-014 [FastComposer: Tuning-Free Multi-subject Image Generation with Localized Attention](https://link.springer.com/content/pdf/10.1007/s11263-024-02227-z.pdf)

**IJCV 2024**

![img](res/T2I%20Applications/1-014-1.png)
![img](res/T2I%20Applications/1-014-2.png)
![img](res/T2I%20Applications/1-014-3.png)

FastComposer is a multi-theme generation method that does not require fine-tuning, using an image encoder to extract theme embeddings for efficient personalized generation.  

</br>


#### 1-015 [InstantID: Zero-shot Identity-Preserving Generation in Seconds](https://arxiv.org/pdf/2401.07519)

**Arxiv 2024**

![img](res/T2I%20Applications/1-015-1.png)
![img](res/T2I%20Applications/1-015-2.png)

InstantID proposed a plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. It designed a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation.  

</br>