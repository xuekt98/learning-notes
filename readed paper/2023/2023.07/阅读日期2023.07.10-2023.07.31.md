# 3D
### NeRF Editing
[FDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models](https://arxiv.org/abs/2306.00783)
来源：Arxiv 2023.06.01  
作者：Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing, Tai Chi-Keung Tang  
机构：Hong Kong University of Science and Technology  
应用：NeRF Editing
简介：本文提出了一种从单张图像重建高质量人脸NeRF的方法，并且具有语义编辑和重新设置照明的功能。[笔记链接]()  

[Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion Model](https://arxiv.org/abs/2306.09551)
来源：Arxiv 2023.06.15
作者：Lu Yu, Wei Xiang, Kang Han
机构：La Trobe University，James Cook University
应用：NeRF Editing
简介：两个阶段模型，第一个阶段固定NeRF训练Diffusion编辑，第二阶段固定Diffusion训练NeRF。[笔记链接]()

[Local 3D Editing via 3D Distillation of CLIP Knowledge](https://arxiv.org/abs/2306.12570)
来源：CVPR2023  
作者：Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo  
机构：KAIST AI  
应用：NeRF Editing  
简介：在EG3D的基础上加入模块来实现基于文本的NeRF编辑。 [笔记链接]()  

# 2D  
### Diffusion Improve Efficiency
[Decoupled Diffusion Models with Explicit Transition Probability](https://arxiv.org/abs/2306.13720)
来源：Arxiv 2023.06.23  
作者：Yuhang Huang, Zheng Qin, Xinwang Liu, Kai Xu  
机构：国防科技大学  
应用：Image Synthesis  
简介：提出了一种将Diffusion过程解耦为图像湮灭和噪声增强的两个过程。 [笔记链接]()  

### 2D Image editing
[Continuous Layout Editing of Single Images with Diffusion Models](https://arxiv.org/abs/2306.13078)
来源：Arxiv 2023.06.22  
作者：Zhiyuan Zhang, Zhitong Huang, Jing Liao   
机构：City University of Hong Kong, China  
应用：Layout-based Image Editing  
简介：本文基于text-based stable diffusion，利用attention map和masked textual inversion实现对于单张输入图片进行layout编辑。 [笔记链接]()

### 2D text-to-Image
[SpaText: Spatio-Textual Representation for Controllable Image Generation](https://arxiv.org/abs/2211.14305)
来源：CVPR 2023  
作者：Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh  
机构：Meta AI  
应用：text-to-image, layout-to-image
简介：本文提出了一种新的文本图像生成方法，这种方法有三个输入，第一个输入文本控制图像全局的内容，第二个是一组文本精细化的描述每一类物体，第三个输入是粗糙的segmentation map，用来大概给出每一类物体所在的布局位置，从而利用这三个输入信息精细化的控制生成图像中的物体属性和位置布局。  [笔记链接]()

[]()
来源：
作者：
机构：
应用：
简介：[笔记链接]()
