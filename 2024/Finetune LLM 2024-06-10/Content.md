# 文章阅读记录

* [<font size=3>**Template**</font>](template)  
<font color=orange>**Origin:**</font>**[[Project Code](https://github.com/SooLab/Free-Bloom)]**  
<font color=orange>**Authors:**</font>  
<font color=orange>**Label:**</font>  
<font color=orange>**Abastract:**</font>  

</br>

[<back to top\>](#文章阅读记录)
# LLM & MLLM

* [<font size=3>**(GPT-1) Improving Language Understanding by Generative Pre-Training**</font>](06.26-07.03/06.26-07.03%20LLM%20Summary.md)  

* [<font size=3>**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**</font>](06.26-07.03/06.26-07.03%20LLM%20Summary.md)   

* [<font size=3>**(GPT-2) Language Models are Unsupervised Multitask Learners**</font>](06.26-07.03/06.26-07.03%20LLM%20Summary.md)  

* [<font size=3>**(GPT-3) Language Models are Few-Shot Learners**</font>](06.26-07.03/06.26-07.03%20LLM%20Summary.md)  

* [<font size=3>**Large Language Models: A Survey**</font>](06.26-07.03/06.26-07.03%20LLM%20Summary.md)  


</br>
</br>


[<back to top\>](#文章阅读记录)
# Image Generation and Editing

## Text-to-Image Diffusion Models

* [<font size=3>**SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis**</font>](08.28-09.04/017%20SDXL.md)  
<font color=orange>**Origin:**</font> ICLR2024 Spotlight; Stability AI; **[[Project Code](https://github.com/Stability-AI/generative-models)]**  
<font color=orange>**Authors:**</font> Aditya Ramesh; Prafulla Dhariwal; Alex Nichol; etc.    
<font color=orange>**Label:**</font> text-to-image; latent diffusion model; stable diffusion;   
<font color=orange>**Abastract:**</font> Stable Diffusion XL; improved latent diffusion model; SDXL leverages a three times larger UNet backbone; a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique.   

* [<font size=3>**DALL-E2 Hierarchical Text-Conditional Image Generation with CLIP Latents**</font>](08.28-09.04/016%20DALLE2.md)  
<font color=orange>**Origin:**</font> Arxiv2204; OpenAI; **[[no Code]()]**  
<font color=orange>**Authors:**</font> Aditya Ramesh; Prafulla Dhariwal; Alex Nichol; etc.    
<font color=orange>**Label:**</font> text-to-image; diffusion model; unCLIP; CLIP   
<font color=orange>**Abastract:**</font> DALL-E2; unCLIP; a prior that generates a CLIP image embedding given a text caption, and a diffusion decoder that generates an image conditioned on the image embedding.  

* [<font size=3>**Imagen: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding**</font>](08.28-09.04/015%20Imagen.md)    
<font color=orange>**Origin:**</font> NIPS2022; Google; **[[Project Code](imagen.research.google)]**  
<font color=orange>**Authors:**</font> Chitwan Saharia, William Chan, etc.    
<font color=orange>**Label:**</font> text-to-image; diffusion model; LLM   
<font color=orange>**Abastract:**</font> Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. 

* [<font size=3>**GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**</font>](08.28-09.04/013%20GLIDE.md)  
<font color=orange>**Origin:**</font> PMLR2022; OpenAI; **[[Project Code](https://github.com/openai/glide-text2im)]**  
<font color=orange>**Authors:**</font> Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, etc.    
<font color=orange>**Label:**</font> text-to-image; diffusion model;   
<font color=orange>**Abastract:**</font> The first text-to-image diffusion model. Pixel space using Classifier-Free Guidance.    

* [<font size=3>**DALL-E: Zero-Shot Text-to-Image Generation**</font>](08.28-09.04/014%20DALLE.md)  
<font color=orange>**Origin:**</font> PMLR2021; OpenAI; **[[Project Code](https://github.com/openai/DALL-E)]**  
<font color=orange>**Authors:**</font> Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, etc.    
<font color=orange>**Label:**</font> text-to-image; transformer;   
<font color=orange>**Abastract:**</font> DALL-E, text-to-image generation using transformer.  

## Personalization

* [<font size=3>**An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion**</font>](08.28-09.04/018%20Textual%20Inversion.md)  
<font color=orange>**Origin:**</font> ICLR2023; Tel-Aviv University, Nvidia; **[[Project Code](https://github.com/rinongal/textual_inversion)]**  
<font color=orange>**Authors:**</font> Rinon Gal; Yuval Alaluf; Yuval Atzmon; etc.    
<font color=orange>**Label:**</font> text-to-image; latent diffusion model; personalization; textual-inversion   
<font color=orange>**Abastract:**</font> using textual inversion to find a pseudo-word to represent a specific concept.   

## Spatial Control

* [<font size=3>**GLIGEN: Open-Set Grounded Text-to-Image Generation**</font>](09.04-09.11/019%20GLIGEN.md)  
<font color=orange>**Origin:**</font> CVPR2023; University of Wisconsin-Madison; Columbia University; Microsoft; **[[Project Code](https://github.com/Stability-AI/generative-models)]**  
<font color=orange>**Authors:**</font>Yuheng Li; Haotian Liu; Qingyang Wu; etc.    
<font color=orange>**Label:**</font> text-to-image; latent diffusion model; conditional generation; layout-to-image   
<font color=orange>**Abastract:**</font>a new text2img generation method that endows new grounding controllability over existing text2img diffusion models by preserving the pre-trained weights and learning to gradually integrate the new localization layers.  



## Non-LLM Based

* [<font size=3>**All are Worth Words: A ViT Backbone for Diffusion Models**</font>](07.03-07.10/010%20All%20are%20worth%20Words.md)  
<font color=orange>**Origin:**</font> CVPR2023; Tsinghua University; **[[Project Code](https://github.com/baofff/U-ViT)]**  
<font color=orange>**Authors:**</font> Fan Bao; Shen Nie; Kaiwen Xue; etc.  
<font color=orange>**Label:**</font> ViT; Diffusion; U-ViT; Unconditional Generation; Conditional Generation  
<font color=orange>**Abastract:**</font> a new Vision Transformer(ViT) based model architecture for Diffusion Models called U-ViT.

</br>

[<back to top\>](#文章阅读记录)
## LLM Based

* [<font size=3>**GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing**</font>](07.10-07.17/011%20GenArtist.md)  
<font color=orange>**Origin:**</font> Arxiv202407; Tsinghua University; **[[Project Code](https://zhenyuw16.github.io/GenArtist_page/)]**    
<font color=orange>**Authors:**</font> Zhengyu Wang; Aoxue Li; Zhengguo Li; etc.    
<font color=orange>**Label:**</font> MLLM; Image Genaration; Image Editing  
<font color=orange>**Abastract:**</font> a unified image generation and editing system, which can satisfy nearly all human requirements while producing reliable image results.

* [<font size=3>**LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models**</font>](08.01-08.07/012%20LMD%20LLM-grounded%20Diffusion.md)  
<font color=orange>**Origin:**</font> TMLR2024; UC Berkley; **[[Project Code](https://llm-grounded-diffusion.github.io/)]**  
<font color=orange>**Authors:**</font> Long Lian; Boyi Li; Adam Yala; Trevor Darrell  
<font color=orange>**Label:**</font> LMD; training free; text-to-image; layout-to-image   
<font color=orange>**Abastract:**</font> In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation.  

</br>
</br>


[<back to top\>](#文章阅读记录)
# Video Generation and Editing 

## zero-shot text-to-video

### LLM Based 

* [<font size=3>**Free-Bloom Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator**</font>](06.19-06.26/001%20Free-Bloom.md)  
<font color=orange>**Origin:**</font> NIPS2023; Shanghai Tech University; **[[Project Code](https://github.com/SooLab/Free-Bloom)]**  
<font color=orange>**Authors:**</font> Hanzhuo Huang; Yufan Feng; Cheng Shi; etc.   
<font color=orange>**Label:**</font> zero-shot; text-to-video; LLM  
<font color=orange>**Abastract:**</font> a zero-shot text-to-video model using LLM as a director and using step-aware attention shift to ensure identical coherence and temporal coherence and introducing a training-free dual-path interpolation strategy for frame interpolation.

</br>  

* [<font size=3>**VideoPoet: A Large Language Model for Zero-Shot Video Generation**</font>](06.26-07.03/008%20VideoPoet.md)  
<font color=orange>**Origin:**</font> ICML2024; Google; [no code]  
<font color=orange>**Authors:**</font> Dan Kondratyuk; Lijun Yu; Xiuye Gu; etc.  
<font color=orange>**Label:**</font> zero-shot; text-to-video; LLM  
<font color=orange>**Abastract:**</font> A method for training a Large Language Model (LLM) specifically for video generation, utilizing tokenized data that incorporates both text-paired and unpaired videos.   

</br>  

* [<font size=3>**FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax**</font>](06.26-07.03/009%20FlowZero.md)  
<font color=orange>**Origin:**</font> Arxiv 2023.11;  University of Technology Sydney; [no code]  
<font color=orange>**Authors:**</font> Yu Lu; Linchao Zhu; Hehe Fan; etc.  
<font color=orange>**Label:**</font> zero-shot; text-to-video; LLM  
<font color=orange>**Abastract:**</font> use LLM to convert text into Dynamic Scene Syntax; self-refinement process ensures better alignment of spatio-temporal layouts with text prompts; adaptively controlled background motion increases the realism of scene and camara motion.  

</br>  
</br>  

[<back to top\>](#文章阅读记录)
### Non-LLM Based

* [<font size=3>**Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators**</font>](06.26-07.03/006%20Text2Video-Zero.md)  
<font color=orange>**Origin:**</font> ICCV2023, Picsart AI Resarch **[[Project Code](https://github.com/Picsart-AI-Research/Text2Video-Zero)]**  
<font color=orange>**Authors:**</font> Levon Khachatryan; Andranik Movsisyan; Vahram Tadevosyan; etc.  
<font color=orange>**Label:**</font> zero-shot; text-to-video; Stable Diffusion  
<font color=orange>**Abastract:**</font> proposed a new problem setting of zero-shot text-to-video synthesis; proposed motion dynamics and cross-frame attention to enforce temporally consistent generation   

</br>  

* [<font size=3>**ControlVideo: Training-free Controllable Text-to-Video Generation**</font>](06.26-07.03/007%20ControlVideo%20Training%20Free%20Text-to-Video.md)  
<font color=orange>**Origin:**</font> ICLR2024, Harbin Institute of Technology **[[Project Code](https://github.com/YBYBZhang/ControlVideo)]**    
<font color=orange>**Authors:**</font> Yabo Zhang; Yuxiang Wei; Dongsheng Jiang; etc.    
<font color=orange>**Label:**</font> zero-shot; text-to-video; Stable Diffusion  
<font color=orange>**Abastract:**</font>  propose a training-free ControlVideo for controllable text-to-video generation based on ControlNet and Stable Diffusion, which consists of the fully cross-frame interaction, interleaved-frame smoother, and hierarchical sampler.   

</br>
</br>



