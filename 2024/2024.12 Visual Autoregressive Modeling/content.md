# 文章阅读记录

* [<font size=3>**Template**</font>](template)  
<font color=orange>**Origin:**</font>**[[Project Code](https://github.com/SooLab/Free-Bloom)]**  
<font color=orange>**Authors:**</font>  
<font color=orange>**Label:**</font>  
<font color=orange>**Abastract:**</font>  

</br>


[<back to top\>](#文章阅读记录)
# Visual Autoregressive Modeling


* [<font size=3>**001($\bigstar\bigstar\bigstar$) Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction**</font>](2024.12.04-2024.12.11/001%20Visual%20Autoregressive%20Modeling.md)  
<font color=orange>**Origin:**</font> NIPS2024; Peking University; Bytedance **[[Project Code](https://github.com/FoundationVision/VAR)]**  
<font color=orange>**Authors:**</font> Keyu Tian, Yi Jiang, Zehuan Yuan; etc.  
<font color=orange>**Label:**</font> Image Generation; Autoregressive Models;    
<font color=orange>**Abastract:**</font> Using multi-scale VQGAN codebook to generate image with autoregressive model. Next-scale prediction.    


* [<font size=3>**002 Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation**</font>](2024.12.04-2024.12.11/002%20Autoregressive%20Model%20Beats%20Diffusion.md)  
<font color=orange>**Origin:**</font> Arxiv 2024.06; The University of Hong Kong; Bytedance **[[Project Code](https://github.com/FoundationVision/LlamaGen)]**  
<font color=orange>**Authors:**</font> Peize Sun; Yi Jiang; Shoufa Chen; etc.  
<font color=orange>**Label:**</font> Image Generation; LlamaGen;  
<font color=orange>**Abastract:**</font> Using Llama structure to generate image, next-token prediction  


* [<font size=3>**003 OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation**</font>](2024.12.04-2024.12.11/003%20OmniTokenizer%20A%20Joint%20Image-Video%20Tokenizer%20for%20Visual%20Generation.md)  
<font color=orange>**Origin:**</font> Arxiv 2024.06; Fudan University; Bytedance **[[Project Code](https://github.com/FoundationVision/OmniTokenizer)]**  
<font color=orange>**Authors:**</font> Junke Wang, Yi Jiang, Zehuan Yuan.  
<font color=orange>**Label:**</font> Image Video tokenizer;  
<font color=orange>**Abastract:**</font> a transformer-based tokenizer for joint image and video tokenization  


* [<font size=3>**004 Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models and Time-Dependent Layer Normalization**</font>](2024.12.04-2024.12.11/004%20Alleviating%20Distortion%20in%20Image%20Generation.md)  
<font color=orange>**Origin:**</font> Arxiv 2024.06; Johns Hopkins University; Bytedance **[[Project Code](https://qihao067.github.io/projects/DiMR)]**  
<font color=orange>**Authors:**</font> Qihao Liu; Zhanpeng Zeng; Ju He; etc.  
<font color=orange>**Label:**</font> Diffusion model; multi-resolution  
<font color=orange>**Abastract:**</font> augmenting the Diffusion model with the Multi-Resolution network (DiMR), a framework that refines features across multiple resolutions, progressively enhancing detail from low to high resolution.  


* [<font size=3>**005($\bigstar\bigstar\bigstar$) CONTROLVAR: EXPLORING CONTROLLABLE VISUAL AUTOREGRESSIVE MODELING**</font>](2024.12.04-2024.12.11/004%20Alleviating%20Distortion%20in%20Image%20Generation.md)  
<font color=orange>**Origin:**</font> Arxiv 2024.10; Carnegie Mellon University; Adobe Research; **[[Project Code](https://github.com/lxa9867/ControlVAR)]**  
<font color=orange>**Authors:**</font> Xiang Li, Kai Qiu, Hao Chen; etc.  
<font color=orange>**Label:**</font> VAR; Controllable Generation  
<font color=orange>**Abastract:**</font> ControlVAR jointly models the distribution of image and pixel-level conditions during training and imposes conditional controls during testing. To enhance the joint modeling, we adopt the next-scale AR prediction paradigm and unify control and image representations. A teacher-forcing guidance strategy is proposed to further facilitate controllable generation with joint modeling.  


* [<font size=3>**006 Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%**</font>](2024.12.04-2024.12.11/006%20Scaling%20the%20Codebook%20Size%20of%20VQGAN.md)  
<font color=orange>**Origin:**</font> Arxiv 2024.06; Peking University Microsoft Research Asia **[[Project Code](https://github.com/zh460045050/VQGAN-LC)]**  
<font color=orange>**Authors:**</font> Lei Zhu; FangyunWei; Yanye Lu; Dong Chen;  
<font color=orange>**Label:**</font> VQGAN; Codebook  
<font color=orange>**Abastract:**</font> Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC.  


* [<font size=3>**007 WAVELETS ARE ALL YOU NEED FOR AUTOREGRESSIVE IMAGE GENERATION**</font>](2024.12.04-2024.12.11/006%20Scaling%20the%20Codebook%20Size%20of%20VQGAN.md)  
<font color=orange>**Origin:**</font> Arxiv 2024.11; **[[No code]()]**  
<font color=orange>**Authors:**</font> WAEL MATTAR, IDAN LEVY, NIR SHARON AND SHAI DEKEL  
<font color=orange>**Label:**</font> Wavelet; Autoregressive Image Generation;  
<font color=orange>**Abastract:**</font> wavelet image encoding; a variant of a language transformer whose architecture is re-designed and optimized for token sequences in this ‘wavelet language’.


